= Microbenchmarking

In this section, we talk about how to measure quick operation right way.

== Why is microbenchmarking hard?

Indeed, microbencmarking is very hard. If an operation takes 10–100ns, the operation measurement is a big challenge. I suggest you to use https://github.com/PerfDotNet/BenchmarkDotNet[BenchmarkDotNet] for your benchmarks. It's a library that can help you to make an honest benchmark and get measurements with good precision. Of course, you can write own benchmark without any additional libraries. In this section, we talk about why it is probably a bad idea and what you should know before start.

=== Know the measurement accuracy

If you want to measure something, first of all, you should choose a right measurement tool. Some people use https://msdn.microsoft.com/library/system.datetime.aspx[DateTime]:

[source,cs]
----
DateTime start = DateTime.Now;
// ...
DateTime finish = DateTime.Now;
TimeSpan duration = finish - start;
----

It is a very bad idea for two reasons:

* `DateTime` has pure accuracy. Sometimes it is ok to use `DateTime` for measurement of long operations (that takes seconds) but you can't get good accuracy with `DateTime` for microoperations (that takes nanoseconds).
* Read the following article: http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time[Falsehoods programmers believe about time]. It should help you to understand that using of `DateTime` is a bad idea for benchmarking. For example, imagine that the system clock turns to the https://en.wikipedia.org/wiki/Daylight_saving_time[summer time] in the middle of your benchmark.

The best choice is https://msdn.microsoft.com/library/system.diagnostics.stopwatch.aspx[Stopwatch]:

[source,cs]
----
var stopwatch = Stopwatch.StartNew();
// ..
stopwatch.Stop();
var duration = stopwatch.ElapsedMilliseconds;
----

TODO: add information about Granularity and Latency.


=== Repeat operation several times

If an operation takes nanoseconds and you try to measure a single run of your operations, you will fail. Even if you try measure to measure several runs of the operation during 1–2 ms, your benchmark doesn't show anything. In this case, the runtime influence and measurement overhead is too big, it spoils all the measurements.

If you want to do a microbenchmark, you should run the operation many times and calculate average time. Desirably, the total measurement time should be at least one second.

=== Make warm-up

If you run your benchmark at the first time, it is called the cold start. It includes big amount of third-party logic: jitting of target methods, assemblies loading, CPU cache warm-up, and so on. All of that can increase the work time and spoil the benchmark results. Thus, you should make warm-up: run your benchmark several times idles. Only then you can perform target runs and measure its time.

=== Repeat benchmark several times

Also you can run your benchmark: results may vary from time to time. At the end, you should take the average time. Another good practice is to run your benchmark in separate CLR instance and collect benchmark results from each instance. It will improve the quality of your benchmarks.

TODO: add example

Statistics is important. You should calculate at least min, max, and standard deviation of your measurements. If the standard deviation is big, you shouldn't use only the average time as a result. Maybe you have some mistakes in your benchmark or the measured operation has no permanent work time.

=== Use separate CLR instances

The best way to make a competition of several benchmark methods is run it using difference CLR instances. In the other case, you can have some troubles. For example, you want to measure the following methods:

[source,cs]
----
interface IFoo
{
  int Inc(int x);
}
class Foo1 : IFoo
{
  public int Inc(int x)
  {
    return x + 1;
  }
}
class Foo2 : IFoo
{
  public int Inc(int x)
  {
    return x + 1;
  }
}
void Run(IFoo foo)
{
  for (int i = 0; i < 1000001; i++)
    foo.Inc(0);
}
void Run1()
{
  Run(new Foo1());    
}
void Run2()
{
  Run(new Foo2());    
}
// Target benchmark methods: Run1, Run2
----

If you measure `Run1` and then `Run2`, the results may differ. I'll try to explain it in simple words. In the first case, there is a single implementation of `IFoo` in the memory. So, JIT can optimize the invocation of the `Inc` method. In the next case, there are two implementation of `IFoo` and the such optimization is impossible. If you measure the target method in a common CLR instance one by one, the results of the competition can be wrong for some JIT implementation. See also: http://blogs.msdn.com/b/vancem/archive/2006/03/13/550529.aspx[Digging into interface calls in the .NET Framework: Stub-based dispatch].

It may seems that the application domains is the solution. Indeed, we can run each benchmark method in a separate application domain. But it is not a good solution in the general case. Recall that all of the app domains http://stackoverflow.com/questions/15246167/does-garbage-collection-happen-at-the-process-level-or-appdomain-level[share] the heap and use common GC. Next, recall that the MS.NET GC is https://msdn.microsoft.com/en-us/library/ee787088.aspx[self-tuning]. So, the execution of the first benchmark method in the first app domain can affect GC and execution of the second benchmark method in the second app domain.

Thus, the best choise for benchmarking is to run each target method in own CLR instance.

=== Avoid side effects

For example, we want to measure the following method:

[source,cs]
----
public double Foo()
{
    double a = 1, b = 1;
    for (int i = 0; i < IterationCount; i++)
        a = a + b;
    return a;
}
----

Someone may want to do it with help of `Stopwatch`:

[source,cs]
----
public double Foo()
{
    double a = 1, b = 1;
    var sw = Stopwatch.StartNew();
    for (int i = 0; i < IterationCount; i++)
        a = a + b;
    sw.Stop();
    Console.WriteLine("Time: " + sw.ElapsedMilliseconds);
    return a;
}
----

It is correct way? No, because the Stopwatch variable can add some side effects to the methods. Let's build it with MS.NET-x86. We get the following asm code for the first case:

[source,asm]
----
fld1  
faddp       st(1),st  
----

JIT-x86 keeps the result in a FPU register. Next, let's look to the asm for the second case:

[source,asm]
----
fld1  
fadd        qword ptr [ebp-14h]  
fstp        qword ptr [ebp-14h]  
----

As we can see, now JIT-x86 keeps the result on the stack instead of FPU registers. It deteriorate the method speed of 4 times.

IMPORTANT: *Conclusion:* any changes of a measured method can significantly affect the method performance. 

=== Think about target method invocation

*Wrong approach 1.* Let's assume that we want to compare performance of two methods:

[source,cs]
----
void Foo1()
{
  // ...
}
void Foo2()
{
  // ...
}
----

Maybe you want to write the code like the following:

[source,cs]
----
// Measure: start
for (int i = 0; i < IterationCount; i++)
  Foo1();
// Measure: end
// Measure: start
for (int i = 0; i < IterationCount; i++)
  Foo2();
// Measure: end
----

In this case, we can have a major issue: one method can be inlined and other method can be not inlined. And it will greatly affect the benchmark results. You can't predict whether it's going to happen. Moreover, different JIT versions have different inlining logics. Let's consider the following method:

[source,cs]
----
int WithStarg(int value)
{
    if (value < 0)
        value = -value;
    return value;
}
----

This method contains the `starg` IL opcode and LegacyJIT-x86 http://aakinshin.net/en/blog/dotnet/inlining-and-starg/[can't inline it], but LegacyJIT-x64 can.

*Wrong approach 2.* Let's talk about code generation. What if we take the following method:

[source,cs]
----
double Foo()
{
    return /* target expression */;
}
----

and instead of benchmark like this

[source,cs]
----
double accumulator = 0;
for (int i = 0; i < IterationCount; i++)
    accumulator += Foo();
----

we will automatically generate a wrapper like the following:

[source,cs]
----
double accumulator = 0;
for (int i = 0; i < IterationCount; i++)
    accumulator += /* target expression */;
----

It is wrong approach too, these scenarios are not equivalent because of http://en.wikipedia.org/wiki/Instruction-level_parallelism[CPU instruction-level parallelism]. If we perform explicit inlining, the CPU can apply additional optimizations and spoil the pure result for single operation.

*BenchmarkDotNet approach*. BenchmarkDotNet creates a delegate for each target method and invoke it. A great fact about delegates: JIT can't inline them. Of course, we have some overhead because of delegates invocation, but BenchmarkDotNet tries to eliminate it.

=== Try different environments

There are big amount of different environments for your .NET program. You can use the x86 platform or the x64 platform. You can use the legacy jit or new modern RyuJIT. You can use different target .NET frameworks or CLR versions. You can run your benchmark with classic Microsoft .NET Framework or Mono or CoreCLR. Don't extrapolate benchmark results for single environment to general behaviour. For example, if you switch legacy JIT-x64 to RyuJIT (it is also x64 for now; .NET Framework 4.6 includes RyuJIT by default), it can significantly affect the results. LegacyJit-x64 and RyuJIT use different logic for performing big amount of optimizations: inlining, array bound check elimination, loop unrolling, and so on. Implementations of BCL classes may also differ. For example, there are http://blogs.msdn.com/b/jankrivanek/archive/2012/11/30/stringbuilder-performance-issues-in-net-4-0-and-4-5.aspx[two different] implementation of StringBuilder in MS.NET (the old implementation was changed in MS.NET Framework 4.0). These implementation has different operation complexity by design. 

=== Beware of loops

Beware of loops inside the target method. For example, let's consider the following code:

[source,cs]
----
for (int i = 0; i < 1000; i++)
    Foo();
----

LegacyJIT-x64 will perform http://en.wikipedia.org/wiki/Loop_unrolling[loop unrolling] and transform the code to the following:

[source,cs]
----
for (int i = 0; i < 1000; i += 4)
{
    Foo();
    Foo();
    Foo();
    Foo();
}
----

For now, LegacyJIT-x86 and RyuJIT can't do it. Such loop unrolling can also spoil the measurement of the `Foo()` invocation.

=== GC

You should control GC overhead and collect the garbage between measurements. The target method shouldn't create objects which can't be collected. A sudden GC stop-the-world can increase time of the target runs. 

=== ProcessorAffinity

For now, BenchmarkDotNet allows you to make only the single thread benchmarks. Multithreading benchmarking is very a hard job, but future plans includes support such kind of benchmarks. Even the single thread benchmarking is the a hard job. For example, you process can be moved from one CPU core to another with a cold processor cache. In this case, results of the measurement will be spoiled. Because of that, BenchmarkDotNet sets https://msdn.microsoft.com/en-us/library/system.diagnostics.process.processoraffinity.aspx[ProcessortAffinity] of the process.  
	
=== Take into account benchmark infrastructure overhead

However, if you try to measure something like this:

[source,cs]
----
for (int i = 0; i < IterationCount; i++)
    Foo();
----

you will actually measure not only the `Foo()` time, but the `Foo()` time plus the `for` time plus the `Foo()` invocations time. It is critical in microbenchmarking. So, you should try to eliminate overhead of your benchmark infrostructure. Fortunately, BenchmarkDotNet tries to do it as much as possible.

=== Conclusion

Thus, hand-writing of the benchmark infrastucture for each benchmark is very hard. Therefore it is best to use a special benchmark library (e.g., *BenchmarkDotNet*) for your experiments.

== Microbenchmarking rules.

Even if you use the BenchmarkDotNet library for benchmarking, there are some rules that you should follow.

=== Use the Release build without an attached debugger

Never use the Debug build for benchmarking. *Never*. The debug version of the target method can run 10–100 times slower. The release mode means that you should have `<Optimize>true</Optimize>` in your csproj file or use https://msdn.microsoft.com/en-us/library/t0hfscdc.aspx[/optimize] for `csc`. Also your never should use an attached debugger (e.g. Visual Studio or WinDbg) during the benchmarking. The best way is build our benchmark in the Release mode and run it with `cmd`.

=== Try different environments

Please, don't extrapolate your results. Or do it very carefully.

I remind you again: the results in different environments may vary significantly. If a `Foo1` method is faster than a `Foo2` method for CLR4, .NET Framework 4.5, x64, RyuJIT, Windows, it means that the `Foo1` method is faster than the `Foo2` method for CLR4, .NET Framework 4.5, x64, RyuJIT, Windows and nothing else. And you can not say anything about methods performance for CLR 2 or .NET Framework 4.6 or LegacyJIT-x64 or x86 or Linux+Mono until you try it. 

=== Avoid dead code elimination

You should also use the result of calculation. For example, if you run the following code:

[source,cs]
----
void Foo()
{
    Math.Exp(1);
}
----

then JIT can eliminate this code because the result of `Math.Exp` is not used. The better way is use it like this:

[source,cs]
----
double Foo()
{
    return Math.Exp(1);
}
----

=== Minimize work with memory

If you don't measure efficiency of access to memory, efficiency of the CPU cache, efficiency of GC, you shouldn't create big arrays and you shouldn't allocate big amount of memory. For example, you want to measure performance of `ConvertAll(x => 2 * x).ToList()`. You can write code like this:

[source,cs]
----
List<int> list = /* ??? */;
public List<int> ConvertAll()
{
    return list.ConvertAll(x => 2 * x).ToList();
}
----

In this case, you should create a small list like this:

[source,cs]
----
List<int> list = new List<int> { 1, 2, 3, 4, 5 };
----

If you create a big list (with millions of elements), then you will also measure efficiency of the CPU cache because you will have big amount of http://en.wikipedia.org/wiki/CPU_cache#Cache_miss[cache miss] during the calculation.  

=== Power settings and other applications

* Turn off all of the applications except the benchmark process and the standard OS processes. If you run benchmark and work in the Visual Studio at the same time, it can negatively affect to benchmark results.
* If you use laptop for benchmarking, keep it plugged in and use the maximum performance mode. 